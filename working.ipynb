{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "from glob import glob \n",
    "import yaml\n",
    "\n",
    "from src.datafactory import dataloader \n",
    "from src.model import Generator,Discriminator,weights_init\n",
    "from src.Network import Encoder,Decoder \n",
    "from src.test import metrics \n",
    "from src.scheduler import CosineAnnealingWarmupRestarts\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc \n",
    "\n",
    "cfg = yaml.load(open('./config/default.yaml','r'), Loader=yaml.FullLoader)\n",
    "\n",
    "device = cfg['TRAIN']['device']\n",
    "\n",
    "trainloader = dataloader(\n",
    "    datadir     = './Data/train_ver2.csv',\n",
    "    window_size = cfg['DATA']['window_size'],\n",
    "    stride      = cfg['DATA']['stride'],\n",
    "    batch_size  = cfg['TRAIN']['batchsize'],\n",
    "    shuffle     = True \n",
    ")\n",
    "testloader = dataloader(\n",
    "    datadir     = './Data/test_ver2.csv',\n",
    "    window_size = cfg['DATA']['window_size'],\n",
    "    stride      = cfg['DATA']['stride'],\n",
    "    batch_size  = cfg['TRAIN']['batchsize'],\n",
    "    shuffle     = False  \n",
    ") \n",
    "\n",
    "linear = nn.Sequential(nn.Linear(5120,256),\n",
    "              nn.BatchNorm1d(256),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(256,1)).to(device)\n",
    "\n",
    "# build model \n",
    "D = Discriminator(\n",
    "    cfg['MODEL']['in_c'],\n",
    "    cfg['MODEL']['hidden_c'],\n",
    "    cfg['MODEL']['latent_c']).apply(weights_init).to(device)\n",
    "G = Generator(\n",
    "    cfg['MODEL']['in_c'],\n",
    "    cfg['MODEL']['hidden_c'],\n",
    "    cfg['MODEL']['latent_c']).apply(weights_init).to(device)\n",
    "\n",
    "# build loss function \n",
    "bce_criterion = nn.BCELoss()\n",
    "mse_criterion = nn.MSELoss()\n",
    "\n",
    "# build optimizer \n",
    "optimizerG = torch.optim.Adam(G.parameters(),lr=cfg['TRAIN']['lr'],betas=(0.999,0.999))\n",
    "optimizerD = torch.optim.Adam(D.parameters(),lr=cfg['TRAIN']['lr'],betas=(0.999,0.999))\n",
    "schedulerG = CosineAnnealingWarmupRestarts(\n",
    "            optimizerG, \n",
    "            first_cycle_steps = cfg['TRAIN']['epochs'],\n",
    "            max_lr            = cfg['TRAIN']['lr'],\n",
    "            min_lr            = cfg['TRAIN']['min_lr'],\n",
    "            warmup_steps      = int(cfg['TRAIN']['epochs'] * cfg['TRAIN']['warmup_ratio'])\n",
    "        )\n",
    "schedulerD = CosineAnnealingWarmupRestarts(\n",
    "            optimizerD, \n",
    "            first_cycle_steps = cfg['TRAIN']['epochs'],\n",
    "            max_lr            = cfg['TRAIN']['lr'],\n",
    "            min_lr            = cfg['TRAIN']['min_lr'],\n",
    "            warmup_steps      = int(cfg['TRAIN']['epochs'] * cfg['TRAIN']['warmup_ratio'])\n",
    "        )\n",
    "class LossMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.loss_g_adv  = 0.0\n",
    "        self.loss_g_rec  = 0.0\n",
    "        self.loss_g      = 0.0 \n",
    "        \n",
    "        self.loss_d_real = 0.0 \n",
    "        self.loss_d_fake = 0.0 \n",
    "        self.loss_d      = 0.0 \n",
    "        \n",
    "        self.count = 0 \n",
    "        \n",
    "    def update(self,log,n=1):\n",
    "        self.count += n \n",
    "        \n",
    "        self.loss_g_adv  += log['loss_g_adv']\n",
    "        self.loss_g_rec  += log['loss_g_rec']\n",
    "        self.loss_g      += log['loss_g'] \n",
    "        \n",
    "        self.loss_d_real += log['loss_d_real'] \n",
    "        self.loss_d_fake += log['loss_d_fake'] \n",
    "        self.loss_d      += log['loss_d'] \n",
    "        \n",
    "    def avg(self):\n",
    "        log = {\n",
    "            'loss_g_adv'  : self.loss_g_adv/self.count,\n",
    "            'loss_g_rec'  : self.loss_g_rec/self.count,\n",
    "            'loss_g'      : self.loss_g    /self.count,\n",
    "            'loss_d_real' : self.loss_d_real/self.count,\n",
    "            'loss_d_fake' : self.loss_d_fake/self.count,\n",
    "            'loss_d'      : self.loss_d    /self.count\n",
    "            }\n",
    "        self.reset()\n",
    "        return log \n",
    "    \n",
    "def updateD(G,D,x,y,bce_criterion):\n",
    "    real_label = 1 #해당 라벨은 생성한 이미지가 진짜인지 가짜인지 판별하는 라벨 \n",
    "    fake_label = 0 \n",
    "    \n",
    "    G.eval()\n",
    "    D.train()\n",
    "    D.zero_grad()\n",
    "    \n",
    "    # Inference \n",
    "    out_d_real,_ = D(x)\n",
    "    with torch.no_grad():\n",
    "        recon_x,_ = G(x)\n",
    "    out_d_fake,_ = D(recon_x)\n",
    "    \n",
    "    loss_d_real = bce_criterion(out_d_real,torch.full((x.shape[0],), real_label, device=device).type(torch.float32))\n",
    "    loss_d_fake = bce_criterion(out_d_fake,torch.full((x.shape[0],), fake_label,device=device).type(torch.float32))\n",
    "    \n",
    "    # loss backward \n",
    "    loss_d = loss_d_real + loss_d_fake \n",
    "    optimizerD.zero_grad()\n",
    "    loss_d.backward()\n",
    "    optimizerD.step()\n",
    "    return loss_d, loss_d_real,loss_d_fake \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[0/100] |G loss : 4.520|D Loss : 0.352|AUROC : 0.645|ACC : 0.459\n",
      "Epoch:[1/100] |G loss : 5.745|D Loss : 0.090|AUROC : 0.643|ACC : 0.435\n",
      "Epoch:[2/100] |G loss : 3.749|D Loss : 0.009|AUROC : 0.647|ACC : 0.432\n",
      "Epoch:[3/100] |G loss : 3.718|D Loss : 0.004|AUROC : 0.667|ACC : 0.432\n",
      "Epoch:[4/100] |G loss : 3.215|D Loss : 0.004|AUROC : 0.668|ACC : 0.428\n",
      "Epoch:[5/100] |G loss : 2.924|D Loss : 0.005|AUROC : 0.420|ACC : 0.211\n",
      "Epoch:[6/100] |G loss : 2.810|D Loss : 0.001|AUROC : 0.361|ACC : 0.175\n",
      "Epoch:[7/100] |G loss : 2.484|D Loss : 0.001|AUROC : 0.342|ACC : 0.114\n",
      "Epoch:[8/100] |G loss : 2.113|D Loss : 0.004|AUROC : 0.548|ACC : 0.270\n",
      "Epoch:[9/100] |G loss : 1.805|D Loss : 0.003|AUROC : 0.610|ACC : 0.370\n",
      "Epoch:[10/100] |G loss : 1.534|D Loss : 0.002|AUROC : 0.627|ACC : 0.404\n",
      "Epoch:[11/100] |G loss : 1.351|D Loss : 0.000|AUROC : 0.635|ACC : 0.420\n",
      "Epoch:[12/100] |G loss : 1.232|D Loss : 0.001|AUROC : 0.640|ACC : 0.399\n",
      "Epoch:[13/100] |G loss : 1.155|D Loss : 0.001|AUROC : 0.650|ACC : 0.424\n",
      "Epoch:[14/100] |G loss : 1.092|D Loss : 0.001|AUROC : 0.686|ACC : 0.506\n",
      "Epoch:[15/100] |G loss : 1.057|D Loss : 0.001|AUROC : 0.715|ACC : 0.522\n",
      "Epoch:[16/100] |G loss : 1.010|D Loss : 0.000|AUROC : 0.723|ACC : 0.540\n",
      "Epoch:[17/100] |G loss : 0.954|D Loss : 0.000|AUROC : 0.728|ACC : 0.547\n",
      "Epoch:[18/100] |G loss : 0.910|D Loss : 0.000|AUROC : 0.734|ACC : 0.547\n",
      "Epoch:[19/100] |G loss : 0.875|D Loss : 0.000|AUROC : 0.738|ACC : 0.548\n",
      "Epoch:[20/100] |G loss : 0.848|D Loss : 0.000|AUROC : 0.739|ACC : 0.546\n",
      "Epoch:[21/100] |G loss : 0.862|D Loss : 0.000|AUROC : 0.738|ACC : 0.544\n",
      "Epoch:[22/100] |G loss : 0.872|D Loss : 0.000|AUROC : 0.737|ACC : 0.543\n",
      "Epoch:[23/100] |G loss : 0.897|D Loss : 0.000|AUROC : 0.734|ACC : 0.544\n",
      "Epoch:[24/100] |G loss : 0.908|D Loss : 0.000|AUROC : 0.731|ACC : 0.544\n",
      "Epoch:[25/100] |G loss : 0.911|D Loss : 0.000|AUROC : 0.731|ACC : 0.541\n",
      "Epoch:[26/100] |G loss : 0.896|D Loss : 0.000|AUROC : 0.732|ACC : 0.531\n",
      "Epoch:[27/100] |G loss : 0.867|D Loss : 0.000|AUROC : 0.733|ACC : 0.524\n",
      "Epoch:[28/100] |G loss : 0.838|D Loss : 0.000|AUROC : 0.730|ACC : 0.504\n",
      "Epoch:[29/100] |G loss : 0.803|D Loss : 0.000|AUROC : 0.726|ACC : 0.495\n",
      "Epoch:[30/100] |G loss : 0.770|D Loss : 0.000|AUROC : 0.718|ACC : 0.485\n",
      "Epoch:[31/100] |G loss : 0.731|D Loss : 0.000|AUROC : 0.713|ACC : 0.480\n",
      "Epoch:[32/100] |G loss : 0.707|D Loss : 0.000|AUROC : 0.707|ACC : 0.480\n",
      "Epoch:[33/100] |G loss : 0.686|D Loss : 0.000|AUROC : 0.705|ACC : 0.479\n",
      "Epoch:[34/100] |G loss : 0.666|D Loss : 0.000|AUROC : 0.705|ACC : 0.485\n",
      "Epoch:[35/100] |G loss : 0.653|D Loss : 0.000|AUROC : 0.706|ACC : 0.488\n",
      "Epoch:[36/100] |G loss : 0.650|D Loss : 0.000|AUROC : 0.708|ACC : 0.489\n",
      "Epoch:[37/100] |G loss : 0.652|D Loss : 0.000|AUROC : 0.709|ACC : 0.491\n",
      "Epoch:[38/100] |G loss : 0.650|D Loss : 0.000|AUROC : 0.710|ACC : 0.492\n",
      "Epoch:[39/100] |G loss : 0.655|D Loss : 0.000|AUROC : 0.711|ACC : 0.492\n",
      "Epoch:[40/100] |G loss : 0.663|D Loss : 0.000|AUROC : 0.712|ACC : 0.493\n",
      "Epoch:[41/100] |G loss : 0.665|D Loss : 0.000|AUROC : 0.713|ACC : 0.494\n",
      "Epoch:[42/100] |G loss : 0.671|D Loss : 0.000|AUROC : 0.714|ACC : 0.495\n",
      "Epoch:[43/100] |G loss : 0.666|D Loss : 0.000|AUROC : 0.714|ACC : 0.495\n",
      "Epoch:[44/100] |G loss : 0.656|D Loss : 0.000|AUROC : 0.714|ACC : 0.495\n",
      "Epoch:[45/100] |G loss : 0.645|D Loss : 0.000|AUROC : 0.714|ACC : 0.495\n",
      "Epoch:[46/100] |G loss : 0.631|D Loss : 0.000|AUROC : 0.715|ACC : 0.496\n",
      "Epoch:[47/100] |G loss : 0.610|D Loss : 0.000|AUROC : 0.713|ACC : 0.495\n",
      "Epoch:[48/100] |G loss : 0.587|D Loss : 0.000|AUROC : 0.713|ACC : 0.496\n",
      "Epoch:[49/100] |G loss : 0.560|D Loss : 0.000|AUROC : 0.715|ACC : 0.497\n",
      "Epoch:[50/100] |G loss : 0.537|D Loss : 0.000|AUROC : 0.715|ACC : 0.497\n",
      "Epoch:[51/100] |G loss : 0.513|D Loss : 0.000|AUROC : 0.712|ACC : 0.497\n",
      "Epoch:[52/100] |G loss : 0.498|D Loss : 0.000|AUROC : 0.714|ACC : 0.498\n",
      "Epoch:[53/100] |G loss : 0.485|D Loss : 0.000|AUROC : 0.714|ACC : 0.499\n",
      "Epoch:[54/100] |G loss : 0.478|D Loss : 0.000|AUROC : 0.716|ACC : 0.498\n",
      "Epoch:[55/100] |G loss : 0.474|D Loss : 0.000|AUROC : 0.718|ACC : 0.498\n",
      "Epoch:[56/100] |G loss : 0.471|D Loss : 0.000|AUROC : 0.720|ACC : 0.498\n",
      "Epoch:[57/100] |G loss : 0.472|D Loss : 0.000|AUROC : 0.722|ACC : 0.501\n",
      "Epoch:[58/100] |G loss : 0.471|D Loss : 0.000|AUROC : 0.724|ACC : 0.503\n",
      "Epoch:[59/100] |G loss : 0.474|D Loss : 0.000|AUROC : 0.725|ACC : 0.505\n",
      "Epoch:[60/100] |G loss : 0.473|D Loss : 0.000|AUROC : 0.728|ACC : 0.513\n",
      "Epoch:[61/100] |G loss : 0.474|D Loss : 0.000|AUROC : 0.727|ACC : 0.511\n",
      "Epoch:[62/100] |G loss : 0.477|D Loss : 0.000|AUROC : 0.733|ACC : 0.509\n",
      "Epoch:[63/100] |G loss : 0.479|D Loss : 0.000|AUROC : 0.732|ACC : 0.508\n",
      "Epoch:[64/100] |G loss : 0.480|D Loss : 0.000|AUROC : 0.736|ACC : 0.506\n",
      "Epoch:[65/100] |G loss : 0.483|D Loss : 0.000|AUROC : 0.739|ACC : 0.510\n",
      "Epoch:[66/100] |G loss : 0.483|D Loss : 0.000|AUROC : 0.742|ACC : 0.517\n",
      "Epoch:[67/100] |G loss : 0.480|D Loss : 0.000|AUROC : 0.747|ACC : 0.519\n",
      "Epoch:[68/100] |G loss : 0.475|D Loss : 0.000|AUROC : 0.747|ACC : 0.524\n",
      "Epoch:[69/100] |G loss : 0.471|D Loss : 0.000|AUROC : 0.751|ACC : 0.528\n",
      "Epoch:[70/100] |G loss : 0.465|D Loss : 0.000|AUROC : 0.753|ACC : 0.531\n",
      "Epoch:[71/100] |G loss : 0.460|D Loss : 0.000|AUROC : 0.748|ACC : 0.528\n",
      "Epoch:[72/100] |G loss : 0.454|D Loss : 0.000|AUROC : 0.742|ACC : 0.518\n",
      "Epoch:[73/100] |G loss : 0.449|D Loss : 0.000|AUROC : 0.737|ACC : 0.526\n",
      "Epoch:[74/100] |G loss : 0.447|D Loss : 0.000|AUROC : 0.733|ACC : 0.526\n",
      "Epoch:[75/100] |G loss : 0.444|D Loss : 0.000|AUROC : 0.730|ACC : 0.536\n",
      "Epoch:[76/100] |G loss : 0.444|D Loss : 0.000|AUROC : 0.727|ACC : 0.554\n",
      "Epoch:[77/100] |G loss : 0.441|D Loss : 0.000|AUROC : 0.729|ACC : 0.562\n",
      "Epoch:[78/100] |G loss : 0.443|D Loss : 0.000|AUROC : 0.737|ACC : 0.569\n",
      "Epoch:[79/100] |G loss : 0.444|D Loss : 0.000|AUROC : 0.741|ACC : 0.571\n",
      "Epoch:[80/100] |G loss : 0.448|D Loss : 0.000|AUROC : 0.742|ACC : 0.573\n",
      "Epoch:[81/100] |G loss : 0.454|D Loss : 0.000|AUROC : 0.745|ACC : 0.575\n",
      "Epoch:[82/100] |G loss : 0.460|D Loss : 0.000|AUROC : 0.745|ACC : 0.577\n",
      "Epoch:[83/100] |G loss : 0.468|D Loss : 0.000|AUROC : 0.746|ACC : 0.579\n",
      "Epoch:[84/100] |G loss : 0.477|D Loss : 0.000|AUROC : 0.747|ACC : 0.580\n",
      "Epoch:[85/100] |G loss : 0.486|D Loss : 0.000|AUROC : 0.746|ACC : 0.581\n",
      "Epoch:[86/100] |G loss : 0.496|D Loss : 0.000|AUROC : 0.746|ACC : 0.578\n",
      "Epoch:[87/100] |G loss : 0.507|D Loss : 0.000|AUROC : 0.747|ACC : 0.578\n",
      "Epoch:[88/100] |G loss : 0.515|D Loss : 0.000|AUROC : 0.747|ACC : 0.578\n",
      "Epoch:[89/100] |G loss : 0.517|D Loss : 0.000|AUROC : 0.746|ACC : 0.576\n",
      "Epoch:[90/100] |G loss : 0.513|D Loss : 0.000|AUROC : 0.744|ACC : 0.572\n",
      "Epoch:[91/100] |G loss : 0.505|D Loss : 0.000|AUROC : 0.742|ACC : 0.570\n",
      "Epoch:[92/100] |G loss : 0.497|D Loss : 0.000|AUROC : 0.742|ACC : 0.568\n",
      "Epoch:[93/100] |G loss : 0.492|D Loss : 0.000|AUROC : 0.742|ACC : 0.568\n",
      "Epoch:[94/100] |G loss : 0.489|D Loss : 0.000|AUROC : 0.743|ACC : 0.568\n",
      "Epoch:[95/100] |G loss : 0.488|D Loss : 0.000|AUROC : 0.744|ACC : 0.573\n",
      "Epoch:[96/100] |G loss : 0.487|D Loss : 0.000|AUROC : 0.743|ACC : 0.577\n",
      "Epoch:[97/100] |G loss : 0.486|D Loss : 0.000|AUROC : 0.743|ACC : 0.576\n",
      "Epoch:[98/100] |G loss : 0.484|D Loss : 0.000|AUROC : 0.742|ACC : 0.576\n",
      "Epoch:[99/100] |G loss : 0.479|D Loss : 0.000|AUROC : 0.740|ACC : 0.575\n"
     ]
    }
   ],
   "source": [
    "real_label = 1 #해당 라벨은 생성한 이미지가 진짜인지 가짜인지 판별하는 라벨 \n",
    "fake_label = 0 \n",
    "\n",
    "for epoch in range(cfg['TRAIN']['epochs']):\n",
    "    #train epoch \n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    loss_meter = LossMeter() \n",
    "    best = 0 \n",
    "    for i,(x,y) in enumerate(trainloader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        \n",
    "        #def  update D \n",
    "        G.eval()\n",
    "        D.train()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # Train with real \n",
    "        out_d_real,_ = D(x)\n",
    "        loss_d_real = bce_criterion(\n",
    "                                    out_d_real,\n",
    "                                    torch.full((x.shape[0],), real_label, device=device).type(torch.float32)\n",
    "                                    )\n",
    "        \n",
    "        # Train with fake \n",
    "        with torch.no_grad():\n",
    "            recon_x,_ = G(x)\n",
    "        out_d_fake,_ = D(recon_x)\n",
    "        loss_d_fake = bce_criterion(\n",
    "                                    out_d_fake,\n",
    "                                    torch.full((x.shape[0],),fake_label,device=device).type(torch.float32)\n",
    "                                    )\n",
    "        \n",
    "        # loss backward \n",
    "        loss_d = loss_d_real + loss_d_fake\n",
    "        optimizerD.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # ! update G \n",
    "        G.train()\n",
    "        D.eval()\n",
    "        G.zero_grad()\n",
    "        \n",
    "        #reconsturction \n",
    "        recon_x,latent_z = G(x)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _,feat_real = D(x) # original feature\n",
    "            out_g, feat_fake = D(recon_x) # reconstruction feature \n",
    "        \n",
    "        # loss \n",
    "        loss_g_adv = mse_criterion(feat_fake,feat_real) # loss for feature matching \n",
    "        loss_g_rec = mse_criterion(recon_x,x) # reconstruction \n",
    "        \n",
    "        # backward \n",
    "        loss_g = loss_g_adv + loss_g_rec * 1 # w_adv = 1 \n",
    "        optimizerG.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        #logging \n",
    "        log = {\n",
    "        'loss_g_adv' : loss_g_adv ,\n",
    "        'loss_g_rec' : loss_g_rec ,\n",
    "        'loss_g'     : loss_g     ,\n",
    "        'loss_d_real': loss_d_real,\n",
    "        'loss_d_fake': loss_d_fake,\n",
    "        'loss_d'     : loss_d     }\n",
    "        loss_meter.update(log)\n",
    "        \n",
    "    #scheduler step \n",
    "    #schedulerD.step()\n",
    "    #schedulerG.step()\n",
    "    \n",
    "    # Epoch evaluate \n",
    "    log = loss_meter.avg()\n",
    "    auroc,acc,y_true,y_pred,thr = metrics(G,testloader,device)\n",
    "    print(f\"Epoch:[{epoch}/{cfg['TRAIN']['epochs']}] |G loss : {log['loss_g']:.3f}|D Loss : {log['loss_d']:.3f}|AUROC : {auroc:.3f}|ACC : {acc:.3f}\")\n",
    "    if acc > best:\n",
    "        torch.save(G,f\"{cfg['SAVE']['savedir']}/best_G.pt\")        \n",
    "        torch.save(D,f\"{cfg['SAVE']['savedir']}/best_D.pt\")\n",
    "        best = acc \n",
    "    \n",
    "torch.save(G,f\"{cfg['SAVE']['savedir']}/last_G.pt\")        \n",
    "torch.save(D,f\"{cfg['SAVE']['savedir']}/last_D.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[0/100] |G loss : 0.477|D Loss : 0.000|AUROC : 0.738|ACC : 0.569\n",
      "Epoch:[1/100] |G loss : 0.473|D Loss : 0.000|AUROC : 0.738|ACC : 0.571\n",
      "Epoch:[2/100] |G loss : 0.473|D Loss : 0.000|AUROC : 0.740|ACC : 0.575\n",
      "Epoch:[3/100] |G loss : 0.480|D Loss : 0.000|AUROC : 0.746|ACC : 0.583\n",
      "Epoch:[4/100] |G loss : 0.490|D Loss : 0.000|AUROC : 0.752|ACC : 0.595\n",
      "Epoch:[5/100] |G loss : 0.505|D Loss : 0.000|AUROC : 0.757|ACC : 0.604\n",
      "Epoch:[6/100] |G loss : 0.517|D Loss : 0.000|AUROC : 0.758|ACC : 0.616\n",
      "Epoch:[7/100] |G loss : 0.528|D Loss : 0.000|AUROC : 0.757|ACC : 0.628\n",
      "Epoch:[8/100] |G loss : 0.539|D Loss : 0.000|AUROC : 0.758|ACC : 0.638\n",
      "Epoch:[9/100] |G loss : 0.546|D Loss : 0.000|AUROC : 0.759|ACC : 0.638\n",
      "Epoch:[10/100] |G loss : 0.556|D Loss : 0.000|AUROC : 0.762|ACC : 0.637\n",
      "Epoch:[11/100] |G loss : 0.560|D Loss : 0.000|AUROC : 0.763|ACC : 0.628\n",
      "Epoch:[12/100] |G loss : 0.561|D Loss : 0.000|AUROC : 0.762|ACC : 0.614\n",
      "Epoch:[13/100] |G loss : 0.548|D Loss : 0.000|AUROC : 0.761|ACC : 0.605\n",
      "Epoch:[14/100] |G loss : 0.530|D Loss : 0.000|AUROC : 0.755|ACC : 0.579\n",
      "Epoch:[15/100] |G loss : 0.514|D Loss : 0.000|AUROC : 0.753|ACC : 0.570\n",
      "Epoch:[16/100] |G loss : 0.501|D Loss : 0.000|AUROC : 0.750|ACC : 0.565\n",
      "Epoch:[17/100] |G loss : 0.495|D Loss : 0.000|AUROC : 0.744|ACC : 0.553\n",
      "Epoch:[18/100] |G loss : 0.495|D Loss : 0.000|AUROC : 0.741|ACC : 0.554\n",
      "Epoch:[19/100] |G loss : 0.500|D Loss : 0.000|AUROC : 0.738|ACC : 0.555\n",
      "Epoch:[20/100] |G loss : 0.513|D Loss : 0.000|AUROC : 0.738|ACC : 0.560\n",
      "Epoch:[21/100] |G loss : 0.523|D Loss : 0.001|AUROC : 0.742|ACC : 0.577\n",
      "Epoch:[22/100] |G loss : 0.522|D Loss : 0.001|AUROC : 0.738|ACC : 0.564\n",
      "Epoch:[23/100] |G loss : 0.509|D Loss : 0.000|AUROC : 0.737|ACC : 0.558\n",
      "Epoch:[24/100] |G loss : 0.483|D Loss : 0.000|AUROC : 0.740|ACC : 0.572\n",
      "Epoch:[25/100] |G loss : 0.456|D Loss : 0.000|AUROC : 0.740|ACC : 0.571\n",
      "Epoch:[26/100] |G loss : 0.432|D Loss : 0.000|AUROC : 0.741|ACC : 0.574\n",
      "Epoch:[27/100] |G loss : 0.410|D Loss : 0.000|AUROC : 0.743|ACC : 0.580\n",
      "Epoch:[28/100] |G loss : 0.391|D Loss : 0.000|AUROC : 0.746|ACC : 0.587\n",
      "Epoch:[29/100] |G loss : 0.377|D Loss : 0.001|AUROC : 0.742|ACC : 0.562\n",
      "Epoch:[30/100] |G loss : 0.365|D Loss : 0.000|AUROC : 0.742|ACC : 0.566\n",
      "Epoch:[31/100] |G loss : 0.355|D Loss : 0.000|AUROC : 0.735|ACC : 0.539\n",
      "Epoch:[32/100] |G loss : 0.345|D Loss : 0.000|AUROC : 0.730|ACC : 0.528\n",
      "Epoch:[33/100] |G loss : 0.337|D Loss : 0.000|AUROC : 0.726|ACC : 0.526\n",
      "Epoch:[34/100] |G loss : 0.328|D Loss : 0.000|AUROC : 0.722|ACC : 0.518\n",
      "Epoch:[35/100] |G loss : 0.321|D Loss : 0.000|AUROC : 0.717|ACC : 0.501\n",
      "Epoch:[36/100] |G loss : 0.316|D Loss : 0.000|AUROC : 0.714|ACC : 0.490\n",
      "Epoch:[37/100] |G loss : 0.312|D Loss : 0.000|AUROC : 0.713|ACC : 0.493\n",
      "Epoch:[38/100] |G loss : 0.310|D Loss : 0.000|AUROC : 0.712|ACC : 0.488\n",
      "Epoch:[39/100] |G loss : 0.307|D Loss : 0.000|AUROC : 0.710|ACC : 0.490\n",
      "Epoch:[40/100] |G loss : 0.305|D Loss : 0.000|AUROC : 0.710|ACC : 0.498\n",
      "Epoch:[41/100] |G loss : 0.301|D Loss : 0.000|AUROC : 0.708|ACC : 0.485\n",
      "Epoch:[42/100] |G loss : 0.298|D Loss : 0.000|AUROC : 0.708|ACC : 0.481\n",
      "Epoch:[43/100] |G loss : 0.296|D Loss : 0.000|AUROC : 0.708|ACC : 0.478\n",
      "Epoch:[44/100] |G loss : 0.293|D Loss : 0.000|AUROC : 0.707|ACC : 0.473\n",
      "Epoch:[45/100] |G loss : 0.289|D Loss : 0.000|AUROC : 0.705|ACC : 0.462\n",
      "Epoch:[46/100] |G loss : 0.286|D Loss : 0.000|AUROC : 0.702|ACC : 0.453\n",
      "Epoch:[47/100] |G loss : 0.284|D Loss : 0.000|AUROC : 0.702|ACC : 0.451\n",
      "Epoch:[48/100] |G loss : 0.282|D Loss : 0.000|AUROC : 0.699|ACC : 0.443\n",
      "Epoch:[49/100] |G loss : 0.283|D Loss : 0.000|AUROC : 0.697|ACC : 0.437\n",
      "Epoch:[50/100] |G loss : 0.282|D Loss : 0.000|AUROC : 0.694|ACC : 0.427\n",
      "Epoch:[51/100] |G loss : 0.281|D Loss : 0.000|AUROC : 0.693|ACC : 0.419\n",
      "Epoch:[52/100] |G loss : 0.280|D Loss : 0.000|AUROC : 0.689|ACC : 0.414\n",
      "Epoch:[53/100] |G loss : 0.277|D Loss : 0.000|AUROC : 0.687|ACC : 0.417\n",
      "Epoch:[54/100] |G loss : 0.275|D Loss : 0.000|AUROC : 0.682|ACC : 0.412\n",
      "Epoch:[55/100] |G loss : 0.271|D Loss : 0.000|AUROC : 0.676|ACC : 0.404\n",
      "Epoch:[56/100] |G loss : 0.268|D Loss : 0.000|AUROC : 0.674|ACC : 0.404\n",
      "Epoch:[57/100] |G loss : 0.267|D Loss : 0.000|AUROC : 0.673|ACC : 0.403\n",
      "Epoch:[58/100] |G loss : 0.267|D Loss : 0.000|AUROC : 0.672|ACC : 0.403\n",
      "Epoch:[59/100] |G loss : 0.265|D Loss : 0.000|AUROC : 0.670|ACC : 0.401\n",
      "Epoch:[60/100] |G loss : 0.262|D Loss : 0.000|AUROC : 0.669|ACC : 0.399\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 79\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m#scheduler step \u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m#schedulerD.step()\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m#schedulerG.step()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \n\u001b[1;32m     77\u001b[0m \u001b[39m# Epoch evaluate \u001b[39;00m\n\u001b[1;32m     78\u001b[0m log \u001b[39m=\u001b[39m loss_meter\u001b[39m.\u001b[39mavg()\n\u001b[0;32m---> 79\u001b[0m auroc,acc,y_true,y_pred,thr \u001b[39m=\u001b[39m metrics(G,testloader,device)\n\u001b[1;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch:[\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mcfg[\u001b[39m'\u001b[39m\u001b[39mTRAIN\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m] |G loss : \u001b[39m\u001b[39m{\u001b[39;00mlog[\u001b[39m'\u001b[39m\u001b[39mloss_g\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m|D Loss : \u001b[39m\u001b[39m{\u001b[39;00mlog[\u001b[39m'\u001b[39m\u001b[39mloss_d\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m|AUROC : \u001b[39m\u001b[39m{\u001b[39;00mauroc\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m|ACC : \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m acc \u001b[39m>\u001b[39m best:\n",
      "File \u001b[0;32m/data/IITP/BeatGAN_mine2/src/test.py:16\u001b[0m, in \u001b[0;36mmetrics\u001b[0;34m(G, testloader, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m score_list \u001b[39m=\u001b[39m [] \n\u001b[1;32m     15\u001b[0m G\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m testloader:\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/IITP/BeatGAN_mine2/src/datafactory.py:18\u001b[0m, in \u001b[0;36mSwatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m,idx):\n\u001b[0;32m---> 18\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49miloc[idx\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride : idx\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow]\u001b[39m.\u001b[39;49mvalues \n\u001b[1;32m     19\u001b[0m     x \u001b[39m=\u001b[39m values[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m     \u001b[39m#if label is last value of Y\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:10877\u001b[0m, in \u001b[0;36mDataFrame.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m  10804\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  10805\u001b[0m \u001b[39mReturn a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[1;32m  10806\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10874\u001b[0m \u001b[39m       ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[1;32m  10875\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m  10876\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m> 10877\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mas_array()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py:1589\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[0;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[1;32m   1587\u001b[0m             arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1588\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1589\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interleave(dtype\u001b[39m=\u001b[39;49mdtype, na_value\u001b[39m=\u001b[39;49mna_value)\n\u001b[1;32m   1590\u001b[0m     \u001b[39m# The underlying data was copied within _interleave\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/internals/managers.py:1654\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[0;34m(self, dtype, na_value)\u001b[0m\n\u001b[1;32m   1652\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m         arr \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mget_values(dtype)\n\u001b[0;32m-> 1654\u001b[0m     result[rl\u001b[39m.\u001b[39;49mindexer] \u001b[39m=\u001b[39m arr\n\u001b[1;32m   1655\u001b[0m     itemmask[rl\u001b[39m.\u001b[39mindexer] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m itemmask\u001b[39m.\u001b[39mall():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "real_label = 1 #해당 라벨은 생성한 이미지가 진짜인지 가짜인지 판별하는 라벨 \n",
    "fake_label = 0 \n",
    "\n",
    "for epoch in range(cfg['TRAIN']['epochs']):\n",
    "    #train epoch \n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    loss_meter = LossMeter() \n",
    "    best = 0 \n",
    "    for i,(x,y) in enumerate(trainloader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        \n",
    "        #def  update D \n",
    "        G.eval()\n",
    "        D.train()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        # Train with real \n",
    "        out_d_real,_ = D(x)\n",
    "        loss_d_real = bce_criterion(\n",
    "                                    out_d_real,\n",
    "                                    torch.full((x.shape[0],), real_label, device=device).type(torch.float32)\n",
    "                                    )\n",
    "        \n",
    "        # Train with fake \n",
    "        with torch.no_grad():\n",
    "            recon_x,_ = G(x)\n",
    "        out_d_fake,_ = D(recon_x)\n",
    "        loss_d_fake = bce_criterion(\n",
    "                                    out_d_fake,\n",
    "                                    torch.full((x.shape[0],),fake_label,device=device).type(torch.float32)\n",
    "                                    )\n",
    "        \n",
    "        # loss backward \n",
    "        loss_d = loss_d_real + loss_d_fake\n",
    "        optimizerD.zero_grad()\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # ! update G \n",
    "        G.train()\n",
    "        D.eval()\n",
    "        G.zero_grad()\n",
    "        \n",
    "        #reconsturction \n",
    "        recon_x,latent_z = G(x)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _,feat_real = D(x) # original feature\n",
    "            out_g, feat_fake = D(recon_x) # reconstruction feature \n",
    "        \n",
    "        # loss \n",
    "        loss_g_adv = mse_criterion(feat_fake,feat_real) # loss for feature matching \n",
    "        loss_g_rec = mse_criterion(recon_x,x) # reconstruction \n",
    "        \n",
    "        # backward \n",
    "        loss_g = loss_g_adv + loss_g_rec * 1 # w_adv = 1 \n",
    "        optimizerG.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        #logging \n",
    "        log = {\n",
    "        'loss_g_adv' : loss_g_adv ,\n",
    "        'loss_g_rec' : loss_g_rec ,\n",
    "        'loss_g'     : loss_g     ,\n",
    "        'loss_d_real': loss_d_real,\n",
    "        'loss_d_fake': loss_d_fake,\n",
    "        'loss_d'     : loss_d     }\n",
    "        loss_meter.update(log)\n",
    "        \n",
    "    #scheduler step \n",
    "    #schedulerD.step()\n",
    "    #schedulerG.step()\n",
    "    \n",
    "    # Epoch evaluate \n",
    "    log = loss_meter.avg()\n",
    "    auroc,acc,y_true,y_pred,thr = metrics(G,testloader,device)\n",
    "    print(f\"Epoch:[{epoch}/{cfg['TRAIN']['epochs']}] |G loss : {log['loss_g']:.3f}|D Loss : {log['loss_d']:.3f}|AUROC : {auroc:.3f}|ACC : {acc:.3f}\")\n",
    "    if acc > best:\n",
    "        torch.save(G,f\"{cfg['SAVE']['savedir']}/best_G.pt\")        \n",
    "        torch.save(D,f\"{cfg['SAVE']['savedir']}/best_D.pt\")\n",
    "        best = acc \n",
    "    \n",
    "torch.save(G,f\"{cfg['SAVE']['savedir']}/last_G.pt\")        \n",
    "torch.save(D,f\"{cfg['SAVE']['savedir']}/last_D.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = [] \n",
    "score_list = [] \n",
    "G.eval()\n",
    "for x,y in testloader:\n",
    "    x = x.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        recon_x,_ = G(x)\n",
    "    anomaly_score = torch.mean(torch.pow((x-recon_x),2),dim=(1,2)).detach().cpu().numpy()\n",
    "    \n",
    "    score_list.extend(anomaly_score)\n",
    "    y_list.extend(y.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7669576897246474, 0.6383751805096195, 0.8262183519509302, 0.5467166679964892]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,f1_score,accuracy_score,recall_score\n",
    "thres = np.percentile(score_list,80)\n",
    "result = [] \n",
    "for me in [precision_score,f1_score,accuracy_score,recall_score]:\n",
    "    re = me(y_list,pd.Series(score_list).apply(lambda x : 1 if x > thres else 0).values)\n",
    "    result.append(re)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def updateG(G,D,x,y,bce_criterion,mse_criterion):\n",
    "    G.train()\n",
    "    D.eval()\n",
    "    G.zero_grad()\n",
    "    \n",
    "    # reconstruction \n",
    "    recon_x,latent_z = G(x)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _,feat_real = D(x)\n",
    "        out_g, feat_fake = D(recon_x)\n",
    "        \n",
    "    # loss \n",
    "    loss_g_adv = mse_criterion(feat_fake,feat_real)\n",
    "    loss_g_rec = mse_criterion(recon_x,x)\n",
    "    \n",
    "    # backward \n",
    "    loss_g = loss_g_adv + loss_g_rec \n",
    "    optimizerG.zero_grad()\n",
    "    loss_g.backward()\n",
    "    optimizerG.step()\n",
    "    \n",
    "    return loss_g,loss_g_adv,loss_g_rec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
